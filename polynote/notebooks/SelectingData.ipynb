{
  "metadata" : {
    "config" : {
      "dependencies" : {
        
      },
      "exclusions" : [
      ],
      "repositories" : [
        {
          "maven" : {
            "base" : "https://oss.sonatype.org/content/repositories/snapshots/"
          }
        }
      ],
      "sparkConfig" : {
        "spark.sql.warehouse.dir" : "s3a://data/managed",
        "spark.databricks.delta.snapshotPartitions" : "2",
        "spark.hadoop.fs.s3a.path.style.access" : "true",
        "spark.hadoop.fs.s3a.aws.credentials.provider" : "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider",
        "spark.hadoop.fs.s3a.bucket.data.endpoint" : "http://s3:9000/",
        "spark.master" : "local[*]"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "code",
      "execution_count" : 0,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1710023181688,
          "endTs" : 1710023182892
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "println(\"Hello,\")\r\n",
        "println(s\"versions are: Java=${sys.props(\"java.version\")} Scala=${scala.util.Properties.versionNumberString} Spark=${spark.version}\")\r\n",
        "println(\"enjoy the lab!\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "Hello,\n",
            "versions are: Java=17.0.10 Scala=2.13.6 Spark=3.4.2\n",
            "enjoy the lab!\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 1,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## Show tables with Scala and Spark"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1710023216554,
          "endTs" : 1710023217517
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.catalog.listTables.show(false)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+----+-------------+---------+-----------+---------+-----------+\n",
            "|name|catalog      |namespace|description|tableType|isTemporary|\n",
            "+----+-------------+---------+-----------+---------+-----------+\n",
            "|test|spark_catalog|[default]|null       |EXTERNAL |false      |\n",
            "+----+-------------+---------+-----------+---------+-----------+\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1710023201246,
          "endTs" : 1710023204206
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.sql(\"drop table if exists test\")\r\n",
        "val df = Seq(1,2,3).toDF(\"test\")\r\n",
        "df.write.option(\"path\",\"/mnt/data/testTable\").saveAsTable(\"default.test\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1709846663889,
          "endTs" : 1709846664835
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.hadoop.fs.Path\r\n",
        "val p = new Path(\"s3a://data/test/test.txt\")\r\n",
        "val fs = p.getFileSystem(spark.sparkContext.hadoopConfiguration)\r\n",
        "//fs.mkdirs(p)\r\n",
        "fs.exists(p)"
      ],
      "outputs" : [
        {
          "execution_count" : 4,
          "data" : {
            "text/plain" : [
              "true"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Boolean"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1704132208963,
          "endTs" : 1704132209552
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.sql(\"drop table default.int_departures\")"
      ],
      "outputs" : [
        {
          "ename" : "org.apache.spark.sql.catalyst.analysis.NoSuchTableException",
          "evalue" : "[TABLE_OR_VIEW_NOT_FOUND] The table or view `spark_catalog`.`default`.`int_departures` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.",
          "traceback" : [
            "org.apache.spark.sql.errors.QueryCompilationErrors$,noSuchTableError,QueryCompilationErrors.scala,1247",
            "org.apache.spark.sql.execution.datasources.v2.DropTableExec,run,DropTableExec.scala,41",
            "org.apache.spark.sql.execution.datasources.v2.V2CommandExec,result$lzycompute,V2CommandExec.scala,43",
            "org.apache.spark.sql.execution.datasources.v2.V2CommandExec,result,V2CommandExec.scala,43",
            "org.apache.spark.sql.execution.datasources.v2.V2CommandExec,executeCollect,V2CommandExec.scala,49",
            "org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1,$anonfun$applyOrElse$1,QueryExecution.scala,98",
            "org.apache.spark.sql.execution.SQLExecution$,$anonfun$withNewExecutionId$6,SQLExecution.scala,118",
            "org.apache.spark.sql.execution.SQLExecution$,withSQLConfPropagated,SQLExecution.scala,195",
            "org.apache.spark.sql.execution.SQLExecution$,$anonfun$withNewExecutionId$1,SQLExecution.scala,103",
            "org.apache.spark.sql.SparkSession,withActive,SparkSession.scala,827",
            "org.apache.spark.sql.execution.SQLExecution$,withNewExecutionId,SQLExecution.scala,65",
            "org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1,applyOrElse,QueryExecution.scala,98",
            "org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1,applyOrElse,QueryExecution.scala,94",
            "org.apache.spark.sql.catalyst.trees.TreeNode,$anonfun$transformDownWithPruning$1,TreeNode.scala,512",
            "org.apache.spark.sql.catalyst.trees.CurrentOrigin$,withOrigin,TreeNode.scala,104",
            "org.apache.spark.sql.catalyst.trees.TreeNode,transformDownWithPruning,TreeNode.scala,512",
            "org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning,LogicalPlan.scala,31",
            "org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper,transformDownWithPruning,AnalysisHelper.scala,267",
            "org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper,transformDownWithPruning$,AnalysisHelper.scala,263",
            "org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,transformDownWithPruning,LogicalPlan.scala,31",
            "org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,transformDownWithPruning,LogicalPlan.scala,31",
            "org.apache.spark.sql.catalyst.trees.TreeNode,transformDown,TreeNode.scala,488",
            "org.apache.spark.sql.execution.QueryExecution,eagerlyExecuteCommands,QueryExecution.scala,94",
            "org.apache.spark.sql.execution.QueryExecution,commandExecuted$lzycompute,QueryExecution.scala,81",
            "org.apache.spark.sql.execution.QueryExecution,commandExecuted,QueryExecution.scala,79",
            "org.apache.spark.sql.Dataset,<init>,Dataset.scala,218",
            "org.apache.spark.sql.Dataset$,$anonfun$ofRows$2,Dataset.scala,98",
            "org.apache.spark.sql.SparkSession,withActive,SparkSession.scala,827",
            "org.apache.spark.sql.Dataset$,ofRows,Dataset.scala,95",
            "org.apache.spark.sql.SparkSession,$anonfun$sql$1,SparkSession.scala,640",
            "org.apache.spark.sql.SparkSession,withActive,SparkSession.scala,827",
            "org.apache.spark.sql.SparkSession,sql,SparkSession.scala,630",
            "org.apache.spark.sql.SparkSession,sql,SparkSession.scala,671",
            "notebook0.Cell2$2,<init>,Cell2,1",
            "sun.reflect.NativeConstructorAccessorImpl,newInstance0,NativeConstructorAccessorImpl.java,-2",
            "sun.reflect.NativeConstructorAccessorImpl,newInstance,NativeConstructorAccessorImpl.java,62",
            "sun.reflect.DelegatingConstructorAccessorImpl,newInstance,DelegatingConstructorAccessorImpl.java,45",
            "java.lang.reflect.Constructor,newInstance,Constructor.java,423",
            "polynote.kernel.interpreter.scal.ScalaInterpreter,createInstance,ScalaInterpreter.scala,173",
            "polynote.kernel.interpreter.scal.ScalaInterpreter,$anonfun$runClass$4,ScalaInterpreter.scala,185",
            "zio.blocking.package$Blocking$Service,$anonfun$effectBlockingInterrupt$5,package.scala,126",
            "zio.ZIO$,$anonfun$effectSuspend$1,ZIO.scala,2782",
            "zio.internal.FiberContext,liftedTree1$1,FiberContext.scala,571",
            "zio.internal.FiberContext,evaluateNow,FiberContext.scala,571",
            "zio.internal.FiberContext,$anonfun$fork$17,FiberContext.scala,770",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$3,CellExecutor.scala,34",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "scala.util.DynamicVariable,withValue,DynamicVariable.scala,62",
            "scala.Console$,withErr,Console.scala,196",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$2,CellExecutor.scala,34",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "scala.util.DynamicVariable,withValue,DynamicVariable.scala,62",
            "scala.Console$,withOut,Console.scala,167",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$1,CellExecutor.scala,33",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "polynote.kernel.package$,withContextClassLoader,package.scala,67",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,run,CellExecutor.scala,31",
            "java.util.concurrent.ThreadPoolExecutor,runWorker,ThreadPoolExecutor.java,1149",
            "java.util.concurrent.ThreadPoolExecutor$Worker,run,ThreadPoolExecutor.java,624",
            "java.lang.Thread,run,Thread.java,750"
          ],
          "output_type" : "error"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1704031143791,
          "endTs" : 1704031150832
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.sql(\"select * from default.btl_distances where estarrivalairport = 'EDDR'\").show"
      ],
      "outputs" : [
        {
          "ename" : "org.apache.spark.sql.AnalysisException",
          "evalue" : "[TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`btl_distances` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'Filter ('estarrivalairport = EDDR)\n   +- 'UnresolvedRelation [default, btl_distances], [], false\n",
          "traceback" : [
            "org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt,tableNotFound,package.scala,87",
            "org.apache.spark.sql.catalyst.analysis.CheckAnalysis,$anonfun$checkAnalysis0$1,CheckAnalysis.scala,183",
            "org.apache.spark.sql.catalyst.analysis.CheckAnalysis,$anonfun$checkAnalysis0$1$adapted,CheckAnalysis.scala,163",
            "org.apache.spark.sql.catalyst.trees.TreeNode,foreachUp,TreeNode.scala,295",
            "org.apache.spark.sql.catalyst.trees.TreeNode,$anonfun$foreachUp$1,TreeNode.scala,294",
            "org.apache.spark.sql.catalyst.trees.TreeNode,$anonfun$foreachUp$1$adapted,TreeNode.scala,294",
            "scala.collection.Iterator,foreach,Iterator.scala,943",
            "scala.collection.Iterator,foreach$,Iterator.scala,943",
            "scala.collection.AbstractIterator,foreach,Iterator.scala,1431",
            "scala.collection.IterableLike,foreach,IterableLike.scala,74",
            "scala.collection.IterableLike,foreach$,IterableLike.scala,73",
            "scala.collection.AbstractIterable,foreach,Iterable.scala,56",
            "org.apache.spark.sql.catalyst.trees.TreeNode,foreachUp,TreeNode.scala,294",
            "org.apache.spark.sql.catalyst.trees.TreeNode,$anonfun$foreachUp$1,TreeNode.scala,294",
            "org.apache.spark.sql.catalyst.trees.TreeNode,$anonfun$foreachUp$1$adapted,TreeNode.scala,294",
            "scala.collection.Iterator,foreach,Iterator.scala,943",
            "scala.collection.Iterator,foreach$,Iterator.scala,943",
            "scala.collection.AbstractIterator,foreach,Iterator.scala,1431",
            "scala.collection.IterableLike,foreach,IterableLike.scala,74",
            "scala.collection.IterableLike,foreach$,IterableLike.scala,73",
            "scala.collection.AbstractIterable,foreach,Iterable.scala,56",
            "org.apache.spark.sql.catalyst.trees.TreeNode,foreachUp,TreeNode.scala,294",
            "org.apache.spark.sql.catalyst.analysis.CheckAnalysis,checkAnalysis0,CheckAnalysis.scala,163",
            "org.apache.spark.sql.catalyst.analysis.CheckAnalysis,checkAnalysis0$,CheckAnalysis.scala,160",
            "org.apache.spark.sql.catalyst.analysis.Analyzer,checkAnalysis0,Analyzer.scala,188",
            "org.apache.spark.sql.catalyst.analysis.CheckAnalysis,checkAnalysis,CheckAnalysis.scala,156",
            "org.apache.spark.sql.catalyst.analysis.CheckAnalysis,checkAnalysis$,CheckAnalysis.scala,146",
            "org.apache.spark.sql.catalyst.analysis.Analyzer,checkAnalysis,Analyzer.scala,188",
            "org.apache.spark.sql.catalyst.analysis.Analyzer,$anonfun$executeAndCheck$1,Analyzer.scala,211",
            "org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$,markInAnalyzer,AnalysisHelper.scala,330",
            "org.apache.spark.sql.catalyst.analysis.Analyzer,executeAndCheck,Analyzer.scala,208",
            "org.apache.spark.sql.execution.QueryExecution,$anonfun$analyzed$1,QueryExecution.scala,76",
            "org.apache.spark.sql.catalyst.QueryPlanningTracker,measurePhase,QueryPlanningTracker.scala,111",
            "org.apache.spark.sql.execution.QueryExecution,$anonfun$executePhase$2,QueryExecution.scala,202",
            "org.apache.spark.sql.execution.QueryExecution$,withInternalError,QueryExecution.scala,526",
            "org.apache.spark.sql.execution.QueryExecution,$anonfun$executePhase$1,QueryExecution.scala,202",
            "org.apache.spark.sql.SparkSession,withActive,SparkSession.scala,827",
            "org.apache.spark.sql.execution.QueryExecution,executePhase,QueryExecution.scala,201",
            "org.apache.spark.sql.execution.QueryExecution,analyzed$lzycompute,QueryExecution.scala,76",
            "org.apache.spark.sql.execution.QueryExecution,analyzed,QueryExecution.scala,74",
            "org.apache.spark.sql.execution.QueryExecution,assertAnalyzed,QueryExecution.scala,66",
            "org.apache.spark.sql.Dataset$,$anonfun$ofRows$2,Dataset.scala,97",
            "org.apache.spark.sql.SparkSession,withActive,SparkSession.scala,827",
            "org.apache.spark.sql.Dataset$,ofRows,Dataset.scala,95",
            "org.apache.spark.sql.SparkSession,$anonfun$sql$1,SparkSession.scala,640",
            "org.apache.spark.sql.SparkSession,withActive,SparkSession.scala,827",
            "org.apache.spark.sql.SparkSession,sql,SparkSession.scala,630",
            "org.apache.spark.sql.SparkSession,sql,SparkSession.scala,671",
            "notebook0.Cell3$2,<init>,Cell3,1",
            "sun.reflect.NativeConstructorAccessorImpl,newInstance0,NativeConstructorAccessorImpl.java,-2",
            "sun.reflect.NativeConstructorAccessorImpl,newInstance,NativeConstructorAccessorImpl.java,62",
            "sun.reflect.DelegatingConstructorAccessorImpl,newInstance,DelegatingConstructorAccessorImpl.java,45",
            "java.lang.reflect.Constructor,newInstance,Constructor.java,423",
            "polynote.kernel.interpreter.scal.ScalaInterpreter,createInstance,ScalaInterpreter.scala,173",
            "polynote.kernel.interpreter.scal.ScalaInterpreter,$anonfun$runClass$4,ScalaInterpreter.scala,185",
            "zio.blocking.package$Blocking$Service,$anonfun$effectBlockingInterrupt$5,package.scala,126",
            "zio.ZIO$,$anonfun$effectSuspend$1,ZIO.scala,2782",
            "zio.internal.FiberContext,liftedTree1$1,FiberContext.scala,571",
            "zio.internal.FiberContext,evaluateNow,FiberContext.scala,571",
            "zio.internal.FiberContext,$anonfun$fork$17,FiberContext.scala,770",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$3,CellExecutor.scala,34",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "scala.util.DynamicVariable,withValue,DynamicVariable.scala,62",
            "scala.Console$,withErr,Console.scala,196",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$2,CellExecutor.scala,34",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "scala.util.DynamicVariable,withValue,DynamicVariable.scala,62",
            "scala.Console$,withOut,Console.scala,167",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$1,CellExecutor.scala,33",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "polynote.kernel.package$,withContextClassLoader,package.scala,67",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,run,CellExecutor.scala,31",
            "java.util.concurrent.ThreadPoolExecutor,runWorker,ThreadPoolExecutor.java,1149",
            "java.util.concurrent.ThreadPoolExecutor$Worker,run,ThreadPoolExecutor.java,624",
            "java.lang.Thread,run,Thread.java,750"
          ],
          "output_type" : "error"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 7,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1696248377398,
          "endTs" : 1696248383704
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.table(\"default.int_departures\").where($\"estarrivalairport\"===\"LIMC\").show"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-----------------------------+--------+-------------------------------+-----------------+------------------------------+-----------------------------+-------------------+--------------------------------+-------------------------------+----------+------+----------+--------+--------------------+\n",
            "|arrivalAirportCandidatesCount|callsign|departureAirportCandidatesCount|estArrivalAirport|estArrivalAirportHorizDistance|estArrivalAirportVertDistance|estDepartureAirport|estDepartureAirportHorizDistance|estDepartureAirportVertDistance| firstSeen|icao24|  lastSeen|      dt|      dl_ts_captured|\n",
            "+-----------------------------+--------+-------------------------------+-----------------+------------------------------+-----------------------------+-------------------+--------------------------------+-------------------------------+----------+------+----------+--------+--------------------+\n",
            "|                            4|SSR07BM |                              0|             LIMC|                          1704|                          146|               LSZB|                             222|                            267|1673877398|300840|1673879133|20230116|2023-08-14 12:29:...|\n",
            "+-----------------------------+--------+-------------------------------+-----------------+------------------------------+-----------------------------+-------------------+--------------------------------+-------------------------------+----------+------+----------+--------+--------------------+\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 8,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## Select data by using DataObjects configured in SmartDataLake"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 9,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1704183676076,
          "endTs" : 1704183676480
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// import smartdatalake\n",
        "import io.smartdatalake.config.SdlConfigObject.stringToDataObjectId\n",
        "import io.smartdatalake.config.ConfigToolbox\n",
        "import io.smartdatalake.workflow.dataobject._\n",
        "import io.smartdatalake.workflow.ActionPipelineContext\n",
        "import io.smartdatalake.workflow.action.SDLExecutionId\n",
        "import io.smartdatalake.app.SmartDataLakeBuilderConfig\n",
        "import io.smartdatalake.workflow.ExecutionPhase\n",
        "implicit val ss = spark // make Spark session available implicitly"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 10,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1704183677963,
          "endTs" : 1704183685667
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// read config from mounted directory\n",
        "val (registry, globalConfig) = ConfigToolbox.loadAndParseConfig(Seq(\"/mnt/config\"), Some(this.getClass.getClassLoader))\n",
        "// Create the context used by SDL objects\n",
        "implicit val context = ConfigToolbox.getDefaultActionPipelineContext(spark, registry)"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 11,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1704183685679,
          "endTs" : 1704183687778
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// get a dataobject\n",
        "val dataIntAirports = registry.get[DeltaLakeTableDataObject](\"int-airports\")\n",
        "val dataIntDepartures = registry.get[DeltaLakeTableDataObject](\"int-departures\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 12,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1704192227615,
          "endTs" : 1704192227938
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val df = dataIntAirports.getSparkDataFrame()"
      ],
      "outputs" : [
        {
          "ename" : "org.apache.spark.sql.AnalysisException",
          "evalue" : "[TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`int_airports` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [default, int_airports], [], false\n",
          "traceback" : [
            "org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt,tableNotFound,package.scala,87",
            "org.apache.spark.sql.catalyst.analysis.CheckAnalysis,$anonfun$checkAnalysis0$1,CheckAnalysis.scala,183",
            "org.apache.spark.sql.catalyst.analysis.CheckAnalysis,$anonfun$checkAnalysis0$1$adapted,CheckAnalysis.scala,163",
            "org.apache.spark.sql.catalyst.trees.TreeNode,foreachUp,TreeNode.scala,295",
            "org.apache.spark.sql.catalyst.analysis.CheckAnalysis,checkAnalysis0,CheckAnalysis.scala,163",
            "org.apache.spark.sql.catalyst.analysis.CheckAnalysis,checkAnalysis0$,CheckAnalysis.scala,160",
            "org.apache.spark.sql.catalyst.analysis.Analyzer,checkAnalysis0,Analyzer.scala,188",
            "org.apache.spark.sql.catalyst.analysis.CheckAnalysis,checkAnalysis,CheckAnalysis.scala,156",
            "org.apache.spark.sql.catalyst.analysis.CheckAnalysis,checkAnalysis$,CheckAnalysis.scala,146",
            "org.apache.spark.sql.catalyst.analysis.Analyzer,checkAnalysis,Analyzer.scala,188",
            "org.apache.spark.sql.catalyst.analysis.Analyzer,$anonfun$executeAndCheck$1,Analyzer.scala,211",
            "org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$,markInAnalyzer,AnalysisHelper.scala,330",
            "org.apache.spark.sql.catalyst.analysis.Analyzer,executeAndCheck,Analyzer.scala,208",
            "org.apache.spark.sql.execution.QueryExecution,$anonfun$analyzed$1,QueryExecution.scala,76",
            "org.apache.spark.sql.catalyst.QueryPlanningTracker,measurePhase,QueryPlanningTracker.scala,111",
            "org.apache.spark.sql.execution.QueryExecution,$anonfun$executePhase$2,QueryExecution.scala,202",
            "org.apache.spark.sql.execution.QueryExecution$,withInternalError,QueryExecution.scala,526",
            "org.apache.spark.sql.execution.QueryExecution,$anonfun$executePhase$1,QueryExecution.scala,202",
            "org.apache.spark.sql.SparkSession,withActive,SparkSession.scala,827",
            "org.apache.spark.sql.execution.QueryExecution,executePhase,QueryExecution.scala,201",
            "org.apache.spark.sql.execution.QueryExecution,analyzed$lzycompute,QueryExecution.scala,76",
            "org.apache.spark.sql.execution.QueryExecution,analyzed,QueryExecution.scala,74",
            "org.apache.spark.sql.execution.QueryExecution,assertAnalyzed,QueryExecution.scala,66",
            "org.apache.spark.sql.Dataset$,$anonfun$ofRows$1,Dataset.scala,89",
            "org.apache.spark.sql.SparkSession,withActive,SparkSession.scala,827",
            "org.apache.spark.sql.Dataset$,ofRows,Dataset.scala,87",
            "org.apache.spark.sql.DataFrameReader,table,DataFrameReader.scala,608",
            "org.apache.spark.sql.SparkSession,table,SparkSession.scala,601",
            "io.smartdatalake.workflow.dataobject.DeltaLakeTableDataObject,getSparkDataFrame,DeltaLakeTableDataObject.scala,203",
            "notebook0.Cell31$8,<init>,Cell31,1",
            "sun.reflect.NativeConstructorAccessorImpl,newInstance0,NativeConstructorAccessorImpl.java,-2",
            "sun.reflect.NativeConstructorAccessorImpl,newInstance,NativeConstructorAccessorImpl.java,62",
            "sun.reflect.DelegatingConstructorAccessorImpl,newInstance,DelegatingConstructorAccessorImpl.java,45",
            "java.lang.reflect.Constructor,newInstance,Constructor.java,423",
            "polynote.kernel.interpreter.scal.ScalaInterpreter,createInstance,ScalaInterpreter.scala,173",
            "polynote.kernel.interpreter.scal.ScalaInterpreter,$anonfun$runClass$4,ScalaInterpreter.scala,185",
            "zio.blocking.package$Blocking$Service,$anonfun$effectBlockingInterrupt$5,package.scala,126",
            "zio.ZIO$,$anonfun$effectSuspend$1,ZIO.scala,2782",
            "zio.internal.FiberContext,liftedTree1$1,FiberContext.scala,571",
            "zio.internal.FiberContext,evaluateNow,FiberContext.scala,571",
            "zio.internal.FiberContext,$anonfun$fork$17,FiberContext.scala,770",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$3,CellExecutor.scala,34",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "scala.util.DynamicVariable,withValue,DynamicVariable.scala,62",
            "scala.Console$,withErr,Console.scala,196",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$2,CellExecutor.scala,34",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "scala.util.DynamicVariable,withValue,DynamicVariable.scala,62",
            "scala.Console$,withOut,Console.scala,167",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$1,CellExecutor.scala,33",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "polynote.kernel.package$,withContextClassLoader,package.scala,67",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,run,CellExecutor.scala,31",
            "java.util.concurrent.ThreadPoolExecutor,runWorker,ThreadPoolExecutor.java,1149",
            "java.util.concurrent.ThreadPoolExecutor$Worker,run,ThreadPoolExecutor.java,624",
            "java.lang.Thread,run,Thread.java,750"
          ],
          "output_type" : "error"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 13,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1704190783418,
          "endTs" : 1704190783544
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import scala.collection.JavaConverters._\r\n",
        "context.hadoopConf.getPropsWithPrefix(\"fs.s3a.aws\").asScala\r\n",
        ".foreach(println)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "(.credentials.provider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider)\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 14,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1709763100649,
          "endTs" : 1709763100824
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "context.hadoopConf.set(\"fs.s3a.endpoint\",\"http://s3:8888/\")\r\n",
        "context.hadoopConf.set(\"fs.s3a.secret.key\",\"1234\")\r\n",
        "context.hadoopConf.set(\"fs.s3a.access.key\",\"admin\")\r\n",
        "context.hadoopConf.set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\r\n",
        "context.hadoopConf.set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\r\n",
        "//dataIntAirports.prepare"
      ],
      "outputs" : [
        {
          "execution_count" : 14,
          "data" : {
            "application/json" : [
              {
                "pos" : {
                  "sourceId" : "Cell12",
                  "start" : 0,
                  "end" : 7,
                  "point" : 0
                },
                "msg" : "not found: value context",
                "severity" : 2
              },
              {
                "pos" : {
                  "sourceId" : "Cell12",
                  "start" : 61,
                  "end" : 68,
                  "point" : 61
                },
                "msg" : "not found: value context",
                "severity" : 2
              },
              {
                "pos" : {
                  "sourceId" : "Cell12",
                  "start" : 113,
                  "end" : 120,
                  "point" : 113
                },
                "msg" : "not found: value context",
                "severity" : 2
              },
              {
                "pos" : {
                  "sourceId" : "Cell12",
                  "start" : 166,
                  "end" : 173,
                  "point" : 166
                },
                "msg" : "not found: value context",
                "severity" : 2
              },
              {
                "pos" : {
                  "sourceId" : "Cell12",
                  "start" : 246,
                  "end" : 253,
                  "point" : 246
                },
                "msg" : "not found: value context",
                "severity" : 2
              }
            ],
            "text/plain" : [
              "Error: not found: value context (0)",
              "Error: not found: value context (61)",
              "Error: not found: value context (113)",
              "Error: not found: value context (166)",
              "Error: not found: value context (246)"
            ]
          },
          "metadata" : {
            "rel" : "compiler_errors"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 15,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1704190803647,
          "endTs" : 1704190803727
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import io.smartdatalake.definitions.Environment\r\n",
        "import org.apache.hadoop.fs.Path"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 16,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1704190804912,
          "endTs" : 1704190805038
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val fs = Environment.fileSystemFactory.getFileSystem(new Path(\"s3a://data/int-airports\"), spark.sparkContext.hadoopConfiguration)"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 17,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1704191482854,
          "endTs" : 1704191483014
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val p = new Path(\"s3a://data/int-airports\")\r\n",
        "val fs = p.getFileSystem(context.hadoopConf)\r\n",
        "context.hadoopConf.getPropsWithPrefix(\"fs.s3a.bucket\").asScala\r\n",
        ".foreach(println)\r\n",
        "//fs.exists(p)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "(.data.secret.key,1234)\n",
            "(.data.access.key,admin)\n",
            "(.data.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)\n",
            "(.data.endpoint,http://s3:8888/)\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 18,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1704191420773,
          "endTs" : 1704191422869
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "fs.exists(p)"
      ],
      "outputs" : [
        {
          "ename" : "java.nio.file.AccessDeniedException",
          "evalue" : "s3a://data/int-airports: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider IAMInstanceCredentialsProvider : com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))",
          "traceback" : [
            "org.apache.hadoop.fs.s3a.S3AUtils,translateException,S3AUtils.java,212",
            "org.apache.hadoop.fs.s3a.S3AUtils,translateException,S3AUtils.java,175",
            "org.apache.hadoop.fs.s3a.S3AFileSystem,s3GetFileStatus,S3AFileSystem.java,3799",
            "org.apache.hadoop.fs.s3a.S3AFileSystem,innerGetFileStatus,S3AFileSystem.java,3688",
            "org.apache.hadoop.fs.s3a.S3AFileSystem,lambda$exists$34,S3AFileSystem.java,4703",
            "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding,invokeTrackingDuration,IOStatisticsBinding.java,547",
            "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding,lambda$trackDurationOfOperation$5,IOStatisticsBinding.java,528",
            "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding,trackDuration,IOStatisticsBinding.java,449",
            "org.apache.hadoop.fs.s3a.S3AFileSystem,trackDurationAndSpan,S3AFileSystem.java,2337",
            "org.apache.hadoop.fs.s3a.S3AFileSystem,trackDurationAndSpan,S3AFileSystem.java,2356",
            "org.apache.hadoop.fs.s3a.S3AFileSystem,exists,S3AFileSystem.java,4701",
            "notebook0.Cell30$8,<init>,Cell30,1",
            "sun.reflect.NativeConstructorAccessorImpl,newInstance0,NativeConstructorAccessorImpl.java,-2",
            "sun.reflect.NativeConstructorAccessorImpl,newInstance,NativeConstructorAccessorImpl.java,62",
            "sun.reflect.DelegatingConstructorAccessorImpl,newInstance,DelegatingConstructorAccessorImpl.java,45",
            "java.lang.reflect.Constructor,newInstance,Constructor.java,423",
            "polynote.kernel.interpreter.scal.ScalaInterpreter,createInstance,ScalaInterpreter.scala,173",
            "polynote.kernel.interpreter.scal.ScalaInterpreter,$anonfun$runClass$4,ScalaInterpreter.scala,185",
            "zio.blocking.package$Blocking$Service,$anonfun$effectBlockingInterrupt$5,package.scala,126",
            "zio.ZIO$,$anonfun$effectSuspend$1,ZIO.scala,2782",
            "zio.internal.FiberContext,liftedTree1$1,FiberContext.scala,571",
            "zio.internal.FiberContext,evaluateNow,FiberContext.scala,571",
            "zio.internal.FiberContext,$anonfun$fork$17,FiberContext.scala,770",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$3,CellExecutor.scala,34",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "scala.util.DynamicVariable,withValue,DynamicVariable.scala,62",
            "scala.Console$,withErr,Console.scala,196",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$2,CellExecutor.scala,34",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "scala.util.DynamicVariable,withValue,DynamicVariable.scala,62",
            "scala.Console$,withOut,Console.scala,167",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$1,CellExecutor.scala,33",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "polynote.kernel.package$,withContextClassLoader,package.scala,67",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,run,CellExecutor.scala,31",
            "java.util.concurrent.ThreadPoolExecutor,runWorker,ThreadPoolExecutor.java,1149",
            "java.util.concurrent.ThreadPoolExecutor$Worker,run,ThreadPoolExecutor.java,624",
            "java.lang.Thread,run,Thread.java,750"
          ],
          "output_type" : "error"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 19,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1704191448004,
          "endTs" : 1704191448131
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\r\n",
        "val scp = new SimpleAWSCredentialsProvider(p.toUri, spark.sparkContext.hadoopConfiguration)\r\n",
        "val c = scp.getCredentials\r\n",
        "print(c.getAWSSecretKey)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "1234"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 20,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## Historization of airport data"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 21,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1696248566953,
          "endTs" : 1696248568895
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "dataIntDepartures.dropTable"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 22,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "Start Action historize-airports"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 23,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1696248677905,
          "endTs" : 1696248678812
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "dataIntAirports.getSparkDataFrame().printSchema"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "root\n",
            " |-- ident: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- latitude_deg: string (nullable = true)\n",
            " |-- longitude_deg: string (nullable = true)\n",
            " |-- dl_ts_captured: timestamp (nullable = true)\n",
            " |-- dl_ts_delimited: timestamp (nullable = true)\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 24,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1696248732512,
          "endTs" : 1696248733505
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "dataIntAirports.getSparkDataFrame().orderBy($\"ident\",$\"dl_ts_captured\").show"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-----+--------------------+------------------+-------------------+--------------------+-------------------+\n",
            "|ident|                name|      latitude_deg|      longitude_deg|      dl_ts_captured|    dl_ts_delimited|\n",
            "+-----+--------------------+------------------+-------------------+--------------------+-------------------+\n",
            "|  00A|   Total RF Heliport|         40.070985|         -74.933689|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00AA|Aero B Ranch Airport|         38.704022|        -101.473911|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00AK|        Lowell Field|         59.947733|        -151.692524|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00AL|        Epps Airpark| 34.86479949951172| -86.77030181884766|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00AN|Katmai Lodge Airport|         59.093287|        -156.456699|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00AR|Newport Hospital ...|           35.6087|         -91.254898|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00AS|      Fulton Airport|        34.9428028|        -97.8180194|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00AZ|      Cordes Airport|34.305599212646484|-112.16500091552734|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00CA|Goldstone (GTS) A...|          35.35474|        -116.885329|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00CL| Williams Ag Airport|         39.427188|        -121.763427|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00CN|Kitchen Creek Hel...|        32.7273736|       -116.4597417|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00CO|          Cass Field|         40.622202|        -104.344002|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00FA| Grass Patch Airport| 28.64550018310547| -82.21900177001953|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00FD|  Ringhaver Heliport|           28.8466|         -82.345398|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00FL|   River Oak Airport|27.230899810791016| -80.96920013427734|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00GA|    Lt World Airport| 33.76750183105469| -84.06829833984375|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00GE|    Caffrey Heliport|         33.887982|         -84.736983|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00HI|  Kaupulehu Heliport|         19.832881|        -155.978347|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00ID|Delta Shores Airport|48.145301818847656|-116.21399688720703|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "| 00IG|       Goltl Airport|         39.724028|        -101.395994|2023-10-02 12:10:...|9999-12-31 00:00:00|\n",
            "+-----+--------------------+------------------+-------------------+--------------------+-------------------+\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 25,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1696248782970,
          "endTs" : 1696248783509
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "dataIntAirports.dropTable"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 26,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "Delete all files in data/stg-airport and copy the historical result.csv from the folder data-fallback-download/stg-airport into the folder data/stg-aiport.\n",
        "Now start the action historize-airports. Afterwards, start actions download-airports and historize-airports to download fresh data and build up the airport history."
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 27,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1696248897427,
          "endTs" : 1696248899515
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "dataIntAirports.getSparkDataFrame()\n",
        "  .groupBy($\"ident\").count\n",
        "  .orderBy($\"count\".desc)\n",
        "  .show"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-------+-----+\n",
            "|  ident|count|\n",
            "+-------+-----+\n",
            "|   0OH7|    2|\n",
            "|   MTPX|    2|\n",
            "|   1CL8|    2|\n",
            "|   2LA2|    2|\n",
            "|   36FA|    2|\n",
            "|   8LL0|    2|\n",
            "|AF-0006|    2|\n",
            "|AU-0083|    2|\n",
            "|CA-0259|    2|\n",
            "|   ESSX|    2|\n",
            "|   FBGD|    2|\n",
            "|   GE25|    2|\n",
            "|HU-0043|    2|\n",
            "|   K1O4|    2|\n",
            "|   KAAT|    2|\n",
            "|   KCKA|    2|\n",
            "|KZ-0016|    2|\n",
            "|   LA65|    2|\n",
            "|   LETC|    2|\n",
            "|   LOGP|    2|\n",
            "+-------+-----+\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 28,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1696248908962,
          "endTs" : 1696248909991
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "dataIntAirports.getSparkDataFrame()\n",
        "  .where($\"ident\"===\"CDV3\")\n",
        "  .show(false)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-----+-------------------------------------------------+-------------+--------------+--------------------------+--------------------------+\n",
            "|ident|name                                             |latitude_deg |longitude_deg |dl_ts_captured            |dl_ts_delimited           |\n",
            "+-----+-------------------------------------------------+-------------+--------------+--------------------------+--------------------------+\n",
            "|CDV3 |Charlottetown (Queen Elizabeth Hospital) Heliport|46.2554925916|-63.0988866091|2023-10-02 12:14:02.575663|2023-10-02 12:14:28.086627|\n",
            "|CDV3 |Charlottetown (Queen Elizabeth Hospital) Heliport|46.255493    |-63.098887    |2023-10-02 12:14:28.087627|9999-12-31 00:00:00       |\n",
            "+-----+-------------------------------------------------+-------------+--------------+--------------------------+--------------------------+\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 29,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## Deduplication of flight data"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 30,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1696250272927,
          "endTs" : 1696250273348
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val dataIntDepartures = registry.get[DeltaLakeTableDataObject](\"int-departures\")\n",
        "dataIntDepartures.dropTable"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 31,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "Start Action deduplicate-departures"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 32,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1696250359117,
          "endTs" : 1696250360004
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "dataIntDepartures.getSparkDataFrame().printSchema"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "root\n",
            " |-- arrivalAirportCandidatesCount: long (nullable = true)\n",
            " |-- callsign: string (nullable = true)\n",
            " |-- departureAirportCandidatesCount: long (nullable = true)\n",
            " |-- estArrivalAirport: string (nullable = true)\n",
            " |-- estArrivalAirportHorizDistance: long (nullable = true)\n",
            " |-- estArrivalAirportVertDistance: long (nullable = true)\n",
            " |-- estDepartureAirport: string (nullable = true)\n",
            " |-- estDepartureAirportHorizDistance: long (nullable = true)\n",
            " |-- estDepartureAirportVertDistance: long (nullable = true)\n",
            " |-- firstSeen: long (nullable = true)\n",
            " |-- icao24: string (nullable = true)\n",
            " |-- lastSeen: long (nullable = true)\n",
            " |-- dt: string (nullable = true)\n",
            " |-- dl_ts_captured: timestamp (nullable = true)\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 33,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1696250362051,
          "endTs" : 1696250362667
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "dataIntDepartures.getSparkDataFrame()\n",
        "  .groupBy($\"icao24\", $\"estdepartureairport\", $\"dt\")\n",
        "  .count\n",
        "  .orderBy($\"count\".desc)\n",
        "  .show"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+------+-------------------+--------+-----+\n",
            "|icao24|estdepartureairport|      dt|count|\n",
            "+------+-------------------+--------+-----+\n",
            "|4b43ab|               LSZB|20210829|    3|\n",
            "|4b4b8d|               LSZB|20210829|    3|\n",
            "|4b1b13|               LSZB|20210829|    2|\n",
            "|4b4445|               LSZB|20210829|    2|\n",
            "|4b4442|               LSZB|20210829|    1|\n",
            "|4b43ab|               LSZB|20210830|    1|\n",
            "|346603|               LSZB|20210829|    1|\n",
            "|4b4449|               LSZB|20210829|    1|\n",
            "|4b0f70|               LSZB|20210830|    1|\n",
            "|49d283|               LSZB|20210829|    1|\n",
            "|4b1a01|               LSZB|20210829|    1|\n",
            "|4b4445|               LSZB|20210830|    1|\n",
            "|4b51fa|               LSZB|20210829|    1|\n",
            "|aa0da1|               LSZB|20210829|    1|\n",
            "|4d02d7|               LSZB|20210829|    1|\n",
            "|4b1b12|               LSZB|20210829|    1|\n",
            "|494108|               LSZB|20210829|    1|\n",
            "|4b3049|               LSZB|20210829|    1|\n",
            "|4b1f2f|               LSZB|20210830|    1|\n",
            "|44046b|               LSZB|20210829|    1|\n",
            "+------+-------------------+--------+-----+\n"
          ],
          "output_type" : "stream"
        }
      ]
    }
  ]
}